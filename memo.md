<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script>

# メモ
## chapter 2 「パーセプトロン」
- パーセプトロンとは？
    - 閾値を超えると1になってそれを発火という。
    - 同じ構造のパーセプトロンで AND、ORなどを再現できる。
- 線形性と非線形性
    - 二次元平面で描けばわかる話
- これでコンピュータの動作を再現できる
## chapter 3 「ニューラルネットワーク」
- 前章のパーセプトロンはパラメータを人力で設定していた。
    - これを自動で設定できるようにすることを「学習」という。
- 閾値処理は、「ステップ関数」を通していることと同じ
    - つまり、活性化関数の1つに過ぎない
- 活性化関数
    - シグモイド関数
    $$ h(x) = \frac{1}{1+exp(-x)} $$
    - ステップ関数
    - ReLu関数
    $$ h(x) = \max(0, x) $$
    ![](./ch03/activation_function.png)
    - 両者は似ているが、滑らかさという点で違っている。
    - 線形関数だけを何層も重ねても、線形関数になるから意味ない。非線形関数で活性化することに意味がある。
- 行列の導入
    - パーセプトロンを行列で
    $$ \bold{y} = \bold{W} \bold{x} + \bold{b} $$
- 3層ニューラルネットワークの構築
    - init: パラメータと構成の初期化
    - forward: 順伝播
- 出力層の設計
    - 出力層の活性化関数は、タスクに応じて選ぶ必要がある。
    - 回帰問題: 恒等関数
    - 分類問題: ソフトマックス関数
    $$ y_i = \frac{exp(z_i)}{\sum_{j=1}^{N} exp(z_j)} $$
    - ソフトマックス関数は、出力の合計が1になるように変換する。
    - 実装面での注意
        - オーバーフロー対策として出力の最大値を全体から引いた上で指数計算をする
        - 推論時は、softmax関数を通さずにargmaxで最大値を求める
- 手書き文字認識
    - 推論処理のことを「順方向伝播」という。
    - python の import では、sys.path から探してくるということに注意！
    - 正規化
        - データをある範囲に収めること
        - 前処理のワンパターン
    - バッチ処理
        - 入力をある程度まとめて処理すること
        - 計算が速くなる (行列を使った演算になるから。for文と比べると速い。)
        - ```np.argmax(a, axis=1)``` で、1次元目を軸として最大値を求めることができる。

## chapter 4 「ニューラルネットワークの学習」
- データから学習
    - 画像分類を決定的なアルゴリズムでやるのは厳しい
    -　特徴量を抽出して、それを機械学習で分類する
        - SIFT, SURFなど人が考えた画像特徴量をもとにSVMなどで分類する
    - 画像特徴量を自動で学習するのがニューラルネットワーク
    - 訓練データとテストデータに分けて、テストデータで汎化性能を確認する
- 損失関数
    - 重み探索のための指標
    - 例
        - 2乗和誤差
        $$ L = \frac{1}{2} \sum_{i=1}^{N} (y_i - t_i)^2 $$
        - クロスエントロピー誤差
        $$ L = - \sum_{i=1}^{N} t_i log(y_i) $$
    - ミニバッチ学習
        - 訓練データに対して損失関数を求めるとき、一部のデータを使うこと
        $$ L = \frac{1}{N} \sum_{i=1}^{N} L_i $$
    - ニューラルネットワークの損失関数設定
        - なぜ認識精度を指標にしないのか？
            - パラメータの微分がほとんどの場所で0になってしまうから
            - 認識精度はとびとびの値になる
            - 連続的な値である損失関数を使う
- 数値微分
    - 数値の計算によって微分すること
    - 注意点
        - 微小な値だと丸め誤差が出る
        - 中心差分
        $$ \frac{f(x+h) - f(x-h)}{2h} $$

- 勾配法
    - 勾配を計算する
    - 勾配が示す方法は、各場所において関数の値を最も減らす方向
    - 勾配降下法
        - 勾配を使って、パラメータを更新する
        $$ \bold{W} = \bold{W} - \eta \frac{\partial L}{\partial \bold{W}} $$
        - $\eta$ は学習率

- 学習アルゴリズムの実装
    - 1. ミニバッチ、2. 勾配の算出、3. パラメータ更新、4. 繰り返し
    - ミニバッチで確率的に選んでくることを「確率的勾配降下法」と呼ぶ。(SGD)
    - 実際にこれで2層のMLPを実装して学習してみた
    - 数値微分では、誤差逆伝播法と比べてかなり時間がかかる
