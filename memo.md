<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script>

# メモ
## chapter 2 「パーセプトロン」
- パーセプトロンとは？
    - 閾値を超えると1になってそれを発火という。
    - 同じ構造のパーセプトロンで AND、ORなどを再現できる。
- 線形性と非線形性
    - 二次元平面で描けばわかる話
- これでコンピュータの動作を再現できる
## chapter 3 「ニューラルネットワーク」
- 前章のパーセプトロンはパラメータを人力で設定していた。
    - これを自動で設定できるようにすることを「学習」という。
- 閾値処理は、「ステップ関数」を通していることと同じ
    - つまり、活性化関数の1つに過ぎない
- 活性化関数
    - シグモイド関数
    $$ h(x) = \frac{1}{1+exp(-x)} $$
    - ステップ関数
    - ReLu関数
    $$ h(x) = \max(0, x) $$
    ![](./ch03/activation_function.png)
    - 両者は似ているが、滑らかさという点で違っている。
    - 線形関数だけを何層も重ねても、線形関数になるから意味ない。非線形関数で活性化することに意味がある。
- 行列の導入
    - パーセプトロンを行列で
    $$ \bold{y} = \bold{W} \bold{x} + \bold{b} $$
- 3層ニューラルネットワークの構築
    - init: パラメータと構成の初期化
    - forward: 順伝播
- 出力層の設計
    - 出力層の活性化関数は、タスクに応じて選ぶ必要がある。
    - 回帰問題: 恒等関数
    - 分類問題: ソフトマックス関数
    $$ y_i = \frac{exp(z_i)}{\sum_{j=1}^{N} exp(z_j)} $$
    - ソフトマックス関数は、出力の合計が1になるように変換する。
    - 実装面での注意
        - オーバーフロー対策として出力の最大値を全体から引いた上で指数計算をする
        - 推論時は、softmax関数を通さずにargmaxで最大値を求める
- 手書き文字認識
    - 推論処理のことを「順方向伝播」という。
    - python の import では、sys.path から探してくるということに注意！
    - 正規化
        - データをある範囲に収めること
        - 前処理のワンパターン
    - バッチ処理
        - 入力をある程度まとめて処理すること
        - 計算が速くなる (行列を使った演算になるから。for文と比べると速い。)
        - ```np.argmax(a, axis=1)``` で、1次元目を軸として最大値を求めることができる。